{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= models.vgg19(pretrained=True).features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT= 'content.jpg'\n",
    "STYLE= 'style.jpg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def image_loader(img_path):\n",
    "    image= PIL.Image.open(img_path)\n",
    "    loader= transforms.Compose([\n",
    "        transforms.Resize((512,512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    image= loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "content_img= image_loader(CONTENT)\n",
    "style_img= image_loader(STYLE)\n",
    "\n",
    "generated= content_img.clone().requires_grad_(True).to(device).requires_grad_(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19,self).__init__()\n",
    "        self.req_feat=['0','5','10','19','28']\n",
    "        self.model= models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features= []\n",
    "        for name, layer in enumerate(self.model):\n",
    "            x= layer(x)\n",
    "            if name in self.req_feat:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "class Losses:\n",
    "    def __init__(self, content_img, style_img, generated):\n",
    "        self.content_img= content_img\n",
    "        self.style_img= style_img\n",
    "        self.generated= generated\n",
    "\n",
    "    def content_loss(self):\n",
    "        content_loss= F.mse_loss(self.generated, self.content_img)\n",
    "        return content_loss\n",
    "\n",
    "    def style_loss(self):\n",
    "        style_loss= 0\n",
    "        for layer in range(len(self.style_img)):\n",
    "            style_loss+= F.mse_loss(self.generated[layer], self.style_img[layer])\n",
    "        return style_loss\n",
    "\n",
    "    def total_loss(self):\n",
    "        content_loss= self.content_loss()\n",
    "        style_loss= self.style_loss()\n",
    "        total_loss= content_loss + style_loss\n",
    "        return total_loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1850, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/7000 [01:28<28:30:05, 14.67s/it]"
     ]
    }
   ],
   "source": [
    "model= VGG19()\n",
    "epoch=7000\n",
    "lr=4e-3\n",
    "alpha=8\n",
    "beta=70\n",
    "loss= Losses(content_img, style_img, generated)\n",
    "optima= optim.Adam([generated],lr=lr)\n",
    "\n",
    "for e in tqdm(range(epoch)):\n",
    "   \n",
    "#extracting the features of generated, content and the original required for calculating the loss\n",
    "    gen_features=model(generated)\n",
    "    orig_feautes=model(content_img)\n",
    "    style_featues=model(style_img)\n",
    "    \n",
    "    #iterating over the activation of each layer and calculate the loss and add it to the content and the style loss\n",
    "    total_loss=loss.total_loss()\n",
    "    #optimize the pixel values of the generated image and backpropagate the loss\n",
    "    optima.zero_grad()\n",
    "    total_loss.backward()\n",
    "        \n",
    "    optima.step()\n",
    "\n",
    "    #print the image and save it after each 100 epoch\n",
    "    if(e%700==0):\n",
    "        print(total_loss)\n",
    "        \n",
    "        save_image(generated,\"gen.png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a224f02ef3cb2bac5aaae1e0cc249454b02d2bde6d74586490e2ab12c2fe4622"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venve': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
